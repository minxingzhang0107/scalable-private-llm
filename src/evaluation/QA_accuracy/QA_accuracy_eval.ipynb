{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from nltk import tokenize\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from openai import OpenAI\n",
    "#from background_ufunc import *\n",
    "#import PyPDF2\n",
    "import heapq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('results/QA_accuracy', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccessToken(token='eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IkhTMjNiN0RvN1RjYVUxUm9MSHdwSXEyNFZZZyIsImtpZCI6IkhTMjNiN0RvN1RjYVUxUm9MSHdwSXEyNFZZZyJ9.eyJhdWQiOiJhcGk6Ly82YjEzNjY2OS00NzlkLTRkYWUtOWM3Zi1jOTExZjkzNTA4YmMiLCJpc3MiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC9lNzQxZDcxYy1jNmI2LTQ3YjAtODAzYy0wZjNiMzJiMDc1NTYvIiwiaWF0IjoxNzU5OTUwMTY3LCJuYmYiOjE3NTk5NTAxNjcsImV4cCI6MTc1OTk1NTI2MCwiYWNyIjoiMSIsImFpbyI6IkFYUUFpLzhhQUFBQUtDckROT3lCWE15TUVHRFVycDkrRm02QS9qTzQrK24xeGJNTng3SWs5OC91QWdJQkV3dlEybkNEQmRmQXQ0dnIvRTRKZWJtdjlsZng0c1Fjc1pRYTd6YkUxZGxKbzNsbUNBMWdvVEtrUkQzY0NUUkxvZW9VZnozVnRTL1NhaEd6TVZOdEhJNU0xWEt1NzArQlVxM0NTQT09IiwiYW1yIjpbInB3ZCIsIm1mYSJdLCJhcHBpZCI6IjA0YjA3Nzk1LThkZGItNDYxYS1iYmVlLTAyZjllMWJmN2I0NiIsImFwcGlkYWNyIjoiMCIsImVtYWlsIjoiYWc5OTd4QGF0dC5jb20iLCJmYW1pbHlfbmFtZSI6IkdVSEEiLCJnaXZlbl9uYW1lIjoiQVJJVFJBIiwiZ3JvdXBzIjpbIjRiYTU2YzYzLTgxZjMtNDZkMy1iMmI3LTUxYTQ2Mzc4NDZiNCJdLCJpcGFkZHIiOiIxNDQuMTYwLjUuMTQ5IiwibmFtZSI6IkdVSEEsIEFSSVRSQSIsIm9pZCI6IjI2OTc0NGQzLWI0NDYtNGJkMC1hM2JlLTYzMDRkMDA1NDVlZCIsIm9ucHJlbV9zaWQiOiJTLTEtNS0yMS0yMDU3NDk5MDQ5LTEyODk2NzYyMDgtMTk1OTQzMTY2MC0xMTc4NjY4NiIsInJoIjoiMS5BU2NBSE5kQjU3YkdzRWVBUEE4N01yQjFWbWxtRTJ1ZFI2NU5uSF9KRWZrMUNMd25BQlFuQUEuIiwicm9sZXMiOlsiR2VuZXJhbC5BY2Nlc3MiXSwic2NwIjoiZGVmYXVsdCIsInNpZCI6IjAwOGExZjc5LTMzOTgtN2FhOS0yN2Q4LTlkODc4ZDZhMjgxNyIsInN1YiI6InJON2EtSktFTTgxb1d0WERmVnRZNmt3WFBGbmxmU0ZOM0Zxc3NOckZ0bGciLCJ0aWQiOiJlNzQxZDcxYy1jNmI2LTQ3YjAtODAzYy0wZjNiMzJiMDc1NTYiLCJ1bmlxdWVfbmFtZSI6ImFnOTk3eEBhdHQuY29tIiwidXBuIjoiYWc5OTd4QGF0dC5jb20iLCJ1dGkiOiJ0aGpHSXlCeUEwQ0dTM1ljQ2NUS0FBIiwidmVyIjoiMS4wIiwieG1zX2Z0ZCI6ImIxOTkyOEFIOEgzZ2FTajVTZTd4S2hFYWJwUE9wUkVIYXRURms1eDViMHdCZFhObFlYTjBMV1J6YlhNIn0.RjjWf7ZAEbF9QDGSgBbOIzaWEbxKqtz0AtJ3uLyomuyr2j1T_z94TUNcdos3JDnufZiU7dzERa2T9wzwDKwVzGmn40tk_ITciYVftlnB7hN1UB5EAlpgDmENVu3EaZgFC0RhwIfpnzdXT2SGh_Z-A3eo9sLKTO4Q0bVYjJebEd2xWW29-90ZAgiEUFhyB13xffVQKkaMD9atlfLVp8ZyKcdeSMndQTa3cEVobDyjunsJimMm3qrbesJBhX7HoEFyMOoXq_9O_ZZjdaEq2SM-oVIp0l1ZikLbS93B_21_pJjdD05E4lbfh6qHPGm7nS1MX5vp-aNCO1nnBaVisb3EfA', expires_on=1759955257)\n"
     ]
    }
   ],
   "source": [
    "credential = InteractiveBrowserCredential()\n",
    " \n",
    "scope = [\"\"]\n",
    " \n",
    "token = credential.get_token(*scope)\n",
    " \n",
    "print(token)\n",
    " \n",
    "auth_token = token.token\n",
    "'''\n",
    "liteLLM_model = \"llama-3.2\"\n",
    "client = OpenAI(base_url='https://llama-inference.stage.att.com', api_key=auth_token)\n",
    "'''\n",
    "\n",
    "liteLLM_model=\"gpt-4o-mini\"\n",
    " \n",
    "client = OpenAI(base_url='https://cdo-inference.stage.att.com', api_key=auth_token)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def question_finder(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    liteLLM_model=\"gpt-4o-mini\"\n",
    "    total_entries = len(data)\n",
    "    yes_count = 0\n",
    "\n",
    "    for entry in data:\n",
    "        question = entry['question']\n",
    "        ground_truth = entry['ground_truth_answer']\n",
    "        generated = entry['generated_answer']\n",
    "        \n",
    "        result = check_answer_similarity(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            generated=generated,\n",
    "            LLM=liteLLM_model\n",
    "        )\n",
    "        \n",
    "        if \"yes\" in result.lower():\n",
    "            yes_count += 1\n",
    "        else:\n",
    "            print(f\"Question: {question}\\nGround Truth: {ground_truth}\\nGenerated: {generated}\\nResult: {result}\\n\")\n",
    "\n",
    "    proportion_yes = yes_count / total_entries\n",
    "    print(f\"Proportion of 'yes' responses: {proportion_yes}\")\n",
    "    return proportion_yes,file_name\n",
    "\n",
    "#question_finder(\"results/public/1nn_lm_weird_word_replacement/evaluation_combined_1nn_lm_finetuned_embedding_weird_word_perturbed_generated_answers_threshold_0.1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='gpt-4o', created=1677610602, object='model', owned_by='openai'), Model(id='meta-llama/codellama-70b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='llama-3-patronus-lynx-8b-instruct-v1.1', created=1677610602, object='model', owned_by='openai'), Model(id='llama-3.3-70b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='llama-3.1-8b-instruct', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o-mini', created=1677610602, object='model', owned_by='openai'), Model(id='o1', created=1677610602, object='model', owned_by='openai'), Model(id='granite-guardian-3.3-8b', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4.1', created=1677610602, object='model', owned_by='openai'), Model(id='o3-mini', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4.1-mini', created=1677610602, object='model', owned_by='openai'), Model(id='text-embedding-3-small', created=1677610602, object='model', owned_by='openai'), Model(id='paraphrase-minilm-l6-v2', created=1677610602, object='model', owned_by='openai'), Model(id='text-embedding-3-large', created=1677610602, object='model', owned_by='openai'), Model(id='o1-mini', created=1677610602, object='model', owned_by='openai')], object='list')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 'yes' responses: 0.7692307692307693\n",
      "file3001\n",
      "Proportion of 'yes' responses: 0.7884615384615384\n",
      "file2001\n",
      "Proportion of 'yes' responses: 0.7692307692307693\n",
      "file1001\n",
      "Proportion of 'yes' responses: 0.7884615384615384\n",
      "file1\n",
      "Proportion of 'yes' responses: 0.7692307692307693\n",
      "file2002\n",
      "Proportion of 'yes' responses: 0.7692307692307693\n",
      "file1002\n",
      "Proportion of 'yes' responses: 0.7692307692307693\n",
      "file2\n",
      "Proportion of 'yes' responses: 0.7692307692307693\n",
      "file3002\n",
      "Proportion of 'yes' responses: 0.11\n",
      "file2003\n",
      "Proportion of 'yes' responses: 0.11\n",
      "file3003\n",
      "Proportion of 'yes' responses: 0.11\n",
      "file3\n",
      "Proportion of 'yes' responses: 0.11\n",
      "file1003\n",
      "Proportion of 'yes' responses: 0.62\n",
      "file4\n",
      "Proportion of 'yes' responses: 0.62\n",
      "file1004\n",
      "Proportion of 'yes' responses: 0.63\n",
      "file3004\n",
      "Proportion of 'yes' responses: 0.62\n",
      "file2004\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file2005\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file3005\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file1005\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file5\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file2006\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file1006\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file3006\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file6\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file7\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file2007\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file3007\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file1007\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file8\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file2008\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file1008\n",
      "Proportion of 'yes' responses: 0.89\n",
      "file3008\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file9\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file1009\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file3009\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file2009\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file10\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file1010\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file3010\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file2010\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file11\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file2011\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file1011\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file3011\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file12\n",
      "Results have been saved to 10_6_result_0.csv\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file3012\n",
      "Results have been saved to 10_6_result_3.csv\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file2012\n",
      "Results have been saved to 10_6_result_2.csv\n",
      "Proportion of 'yes' responses: 0.78\n",
      "file1012\n",
      "Results have been saved to 10_6_result_1.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\"\"\"\n",
    "FILE: /Users/ag997x/Library/CloudStorage/OneDrive-AT&TServices,Inc/Desktop/att/results/compare results.ipynb\n",
    "This script provides functionality to evaluate the semantic similarity between answers to a given question using a language model (LLM). \n",
    "It includes a function to compare answers and determine if they are semantically equivalent, as well as a utility to analyze a dataset \n",
    "of questions and answers stored in a JSON file.\n",
    "Functions:\n",
    "----------\n",
    "1. check_answer_similarity(question, ground_truth, generated, LLM, tokens=10):\n",
    "    - Compares the semantic meaning of two answers to a given question.\n",
    "    - Parameters:\n",
    "        - question (str): The question being analyzed.\n",
    "        - ground_truth (str): The reference answer (ground truth).\n",
    "        - generated (str): The generated answer to compare.\n",
    "        - LLM (object): The language model to use for comparison.\n",
    "        - tokens (int, optional): The maximum number of tokens for the LLM response. Default is 10.\n",
    "    - Returns:\n",
    "        - str: 'yes' if the answers are semantically equivalent, 'no' otherwise.\n",
    "2. analyze_similarity(file_name):\n",
    "    - Analyzes a JSON file containing questions, ground truth answers, and generated answers to calculate the proportion of semantically \n",
    "      equivalent answers.\n",
    "    - Parameters:\n",
    "        - file_name (str): The path to the JSON file containing the data.\n",
    "    - Returns:\n",
    "        - float: The proportion of 'yes' responses indicating semantic equivalence.\n",
    "Usage:\n",
    "------\n",
    "- The script reads a JSON file containing a list of questions, ground truth answers, and generated answers.\n",
    "- It uses the `check_answer_similarity` function to evaluate each entry and calculates the proportion of semantically equivalent answers.\n",
    "- Example usage is provided at the end of the script.\n",
    "Dependencies:\n",
    "-------------\n",
    "- Requires the `json` module for reading and parsing JSON files.\n",
    "- Assumes the presence of a language model client (`liteLLM_model`) for generating text completions.\n",
    "Note:\n",
    "-----\n",
    "- Ensure the JSON file is formatted correctly with keys: 'question', 'ground_truth_answer', and 'generated_answer'.\n",
    "- The language model client (`liteLLM_model`) must be properly initialized before running the script.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def check_answer_similarity(question, ground_truth, generated, LLM, tokens=10):\n",
    "    def text_complete(prompt, LLM):\n",
    "        text_completion = client.completions.create(\n",
    "            prompt=prompt,\n",
    "            model=LLM,\n",
    "            max_tokens=tokens\n",
    "        )\n",
    "        text_completion.to_dict()\n",
    "        return text_completion.choices[0].text.strip()\n",
    "\n",
    "    prompt = f\"\"\"You are an expert at comparing answers to a given question. \n",
    "              Your task is to determine if the second answer is correct given that the first answer is the ground truth response, even if phrased differently. \n",
    "              The question is: '{question}'. \n",
    "              The first answer is: '{ground_truth}'. \n",
    "              The second answer is: '{generated}'. \n",
    "              Carefully analyze the semantic meaning of both answers and respond with 'yes' if the second answer is correct as measured \n",
    "              against the first with respect to the question, or 'no' if not. Even if part of second answer relevant to the question is incorrect, \n",
    "                 respond with 'no'. \"\"\"\n",
    "\n",
    "    result = text_complete(prompt, LLM)\n",
    "    return result\n",
    "'''\n",
    "response = check_answer_similarity(\n",
    "    question=\"What city served as Maria Garcia Alvarez's birthplace and what careers do her parents pursue?\",\n",
    "    ground_truth=\"Maria Garcia Alvarez was born in Madrid, Spain. Her father is a midwife and her mother is a civil engineer.\",\n",
    "    generated=\"I apologize for the confusion, but I cannot answer that question directly as I don't have access to specific information about Maria Garcia Alvarez's personal background.\",\n",
    "    LLM=liteLLM_model\n",
    ")\n",
    "# Load the JSON file\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters\n",
    "total_entries = len(data)\n",
    "yes_count = 0\n",
    "\n",
    "# Iterate through each entry in the JSON file\n",
    "for entry in data:\n",
    "    question = entry['question']\n",
    "    ground_truth = entry['ground_truth_answer']\n",
    "    generated = entry['generated_answer']\n",
    "    \n",
    "    # Use the function to check similarity\n",
    "    result = check_answer_similarity(\n",
    "        question=question,\n",
    "        ground_truth=ground_truth,\n",
    "        generated=generated,\n",
    "        LLM=liteLLM_model\n",
    "    )\n",
    "    \n",
    "    # Count 'yes' responses\n",
    "    if result.lower() == 'yes':\n",
    "        yes_count += 1\n",
    "\n",
    "# Calculate the proportion of 'yes' responses\n",
    "proportion_yes = yes_count / total_entries\n",
    "print(f\"Proportion of 'yes' responses: {proportion_yes}\")\n",
    "\n",
    "print(response)\n",
    "'''\n",
    "def analyze_similarity(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    liteLLM_model=\"gpt-4o-mini\"\n",
    "    total_entries = len(data)\n",
    "    yes_count = 0\n",
    "\n",
    "    for entry in data:\n",
    "        question = entry['question']\n",
    "        ground_truth = entry['ground_truth_answer']\n",
    "        generated = entry['generated_answer']\n",
    "        \n",
    "        result = check_answer_similarity(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            generated=generated,\n",
    "            LLM=liteLLM_model\n",
    "        )\n",
    "        \n",
    "        if \"yes\" in result.lower():\n",
    "            yes_count += 1\n",
    "\n",
    "    proportion_yes = yes_count / total_entries\n",
    "    print(f\"Proportion of 'yes' responses: {proportion_yes}\")\n",
    "    return proportion_yes,file_name\n",
    "\n",
    "# Example usage\n",
    "#proportion,file_name = analyze_similarity('data.json')\n",
    "\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "import concurrent.futures\n",
    "\n",
    "def process_j(j):\n",
    "    local_results = []\n",
    "    i = 0 + 1000 * j\n",
    "    for root, dirs, files in os.walk('./results'):\n",
    "        for file in files:\n",
    "            if file.startswith('evaluation') and file.endswith('.json'):\n",
    "                i += 1\n",
    "                file_path = os.path.join(root, file)\n",
    "                proportion, file_name = analyze_similarity(file_path)\n",
    "                local_results.append({'file_name': file_name, 'proportion': proportion})\n",
    "                print(\"file\" + str(i))\n",
    "    output_csv = f'results/QA_accuracy/comparison_results_{j}.csv'\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=['file_name', 'proportion'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(local_results)\n",
    "    print(f\"Results have been saved to {output_csv}\")\n",
    "    return local_results\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_j, j) for j in range(4)]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        results.extend(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./10_6_result/DPSGD/public/lm_only/evaluation_DPSGD_tuned_lm_only_generated_answers.json\n",
      "./10_6_result/DPSGD/public/1nn_lm_DPSGD_tuned_embedding/evaluation_combined_1nn_lm_DPSGD_tuned_generated_answers_threshold_0.4.json\n",
      "./10_6_result/DPSGD/private/tofu/lm_only/evaluation_DPSGD_tuned_lm_only_generated_answers.json\n",
      "./10_6_result/DPSGD/private/tofu/1nn_lm_DPSGD_tuned_embedding/evaluation_combined_1nn_lm_DPSGD_tuned_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/weird_word_replacement_embedding/k_1/evaluation_combined_knn_lm_finetuned_embedding_weird_word_perturbed_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/weird_word_replacement_embedding/k_7/evaluation_combined_knn_lm_finetuned_embedding_weird_word_perturbed_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/weird_word_replacement_embedding/k_5/evaluation_combined_knn_lm_finetuned_embedding_weird_word_perturbed_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/weird_word_replacement_embedding/k_3/evaluation_combined_knn_lm_finetuned_embedding_weird_word_perturbed_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/name_perturbation_embedding/k_1/evaluation_combined_knn_lm_finetuned_embedding_name_perturbation_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/name_perturbation_embedding/k_7/evaluation_combined_knn_lm_finetuned_embedding_name_perturbation_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/name_perturbation_embedding/k_5/evaluation_combined_knn_lm_finetuned_embedding_name_perturbation_generated_answers_threshold_0.4.json\n",
      "./10_6_result/redundancy_analysis/name_perturbation_embedding/k_3/evaluation_combined_knn_lm_finetuned_embedding_name_perturbation_generated_answers_threshold_0.4.json\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for root, dirs, files in os.walk('./results'):\n",
    "    for file in files:\n",
    "        if file.startswith('evaluation') and file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(file_path)\n",
    "            c+=1\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HEART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
