{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "from nltk import tokenize\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from openai import OpenAI\n",
    "#from background_ufunc import *\n",
    "#import PyPDF2\n",
    "import heapq\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('results/QA_accuracy', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = InteractiveBrowserCredential()\n",
    " \n",
    "scope = [\"\"]\n",
    " \n",
    "token = credential.get_token(*scope)\n",
    " \n",
    "print(token)\n",
    " \n",
    "auth_token = token.token\n",
    "'''\n",
    "liteLLM_model = \"llama-3.2\"\n",
    "client = OpenAI(base_url='https://llama-inference.stage.att.com', api_key=auth_token)\n",
    "'''\n",
    "\n",
    "liteLLM_model=\"gpt-4o-mini\"\n",
    " \n",
    "client = OpenAI(base_url='https://cdo-inference.stage.att.com', api_key=auth_token)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def question_finder(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    liteLLM_model=\"gpt-4o-mini\"\n",
    "    total_entries = len(data)\n",
    "    yes_count = 0\n",
    "\n",
    "    for entry in data:\n",
    "        question = entry['question']\n",
    "        ground_truth = entry['ground_truth_answer']\n",
    "        generated = entry['generated_answer']\n",
    "        \n",
    "        result = check_answer_similarity(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            generated=generated,\n",
    "            LLM=liteLLM_model\n",
    "        )\n",
    "        \n",
    "        if \"yes\" in result.lower():\n",
    "            yes_count += 1\n",
    "        else:\n",
    "            print(f\"Question: {question}\\nGround Truth: {ground_truth}\\nGenerated: {generated}\\nResult: {result}\\n\")\n",
    "\n",
    "    proportion_yes = yes_count / total_entries\n",
    "    print(f\"Proportion of 'yes' responses: {proportion_yes}\")\n",
    "    return proportion_yes,file_name\n",
    "\n",
    "#question_finder(\"results/public/1nn_lm_weird_word_replacement/evaluation_combined_1nn_lm_finetuned_embedding_weird_word_perturbed_generated_answers_threshold_0.1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\"\"\"\n",
    "FILE: /Users/ag997x/Library/CloudStorage/OneDrive-AT&TServices,Inc/Desktop/att/results/compare results.ipynb\n",
    "This script provides functionality to evaluate the semantic similarity between answers to a given question using a language model (LLM). \n",
    "It includes a function to compare answers and determine if they are semantically equivalent, as well as a utility to analyze a dataset \n",
    "of questions and answers stored in a JSON file.\n",
    "Functions:\n",
    "----------\n",
    "1. check_answer_similarity(question, ground_truth, generated, LLM, tokens=10):\n",
    "    - Compares the semantic meaning of two answers to a given question.\n",
    "    - Parameters:\n",
    "        - question (str): The question being analyzed.\n",
    "        - ground_truth (str): The reference answer (ground truth).\n",
    "        - generated (str): The generated answer to compare.\n",
    "        - LLM (object): The language model to use for comparison.\n",
    "        - tokens (int, optional): The maximum number of tokens for the LLM response. Default is 10.\n",
    "    - Returns:\n",
    "        - str: 'yes' if the answers are semantically equivalent, 'no' otherwise.\n",
    "2. analyze_similarity(file_name):\n",
    "    - Analyzes a JSON file containing questions, ground truth answers, and generated answers to calculate the proportion of semantically \n",
    "      equivalent answers.\n",
    "    - Parameters:\n",
    "        - file_name (str): The path to the JSON file containing the data.\n",
    "    - Returns:\n",
    "        - float: The proportion of 'yes' responses indicating semantic equivalence.\n",
    "Usage:\n",
    "------\n",
    "- The script reads a JSON file containing a list of questions, ground truth answers, and generated answers.\n",
    "- It uses the `check_answer_similarity` function to evaluate each entry and calculates the proportion of semantically equivalent answers.\n",
    "- Example usage is provided at the end of the script.\n",
    "Dependencies:\n",
    "-------------\n",
    "- Requires the `json` module for reading and parsing JSON files.\n",
    "- Assumes the presence of a language model client (`liteLLM_model`) for generating text completions.\n",
    "Note:\n",
    "-----\n",
    "- Ensure the JSON file is formatted correctly with keys: 'question', 'ground_truth_answer', and 'generated_answer'.\n",
    "- The language model client (`liteLLM_model`) must be properly initialized before running the script.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def check_answer_similarity(question, ground_truth, generated, LLM, tokens=10):\n",
    "    def text_complete(prompt, LLM):\n",
    "        text_completion = client.completions.create(\n",
    "            prompt=prompt,\n",
    "            model=LLM,\n",
    "            max_tokens=tokens\n",
    "        )\n",
    "        text_completion.to_dict()\n",
    "        return text_completion.choices[0].text.strip()\n",
    "\n",
    "    prompt = f\"\"\"You are an expert at comparing answers to a given question. \n",
    "              Your task is to determine if the second answer is correct given that the first answer is the ground truth response, even if phrased differently. \n",
    "              The question is: '{question}'. \n",
    "              The first answer is: '{ground_truth}'. \n",
    "              The second answer is: '{generated}'. \n",
    "              Carefully analyze the semantic meaning of both answers and respond with 'yes' if the second answer is correct as measured \n",
    "              against the first with respect to the question, or 'no' if not. Even if part of second answer relevant to the question is incorrect, \n",
    "                 respond with 'no'. \"\"\"\n",
    "\n",
    "    result = text_complete(prompt, LLM)\n",
    "    return result\n",
    "'''\n",
    "response = check_answer_similarity(\n",
    "    question=\"What city served as Maria Garcia Alvarez's birthplace and what careers do her parents pursue?\",\n",
    "    ground_truth=\"Maria Garcia Alvarez was born in Madrid, Spain. Her father is a midwife and her mother is a civil engineer.\",\n",
    "    generated=\"I apologize for the confusion, but I cannot answer that question directly as I don't have access to specific information about Maria Garcia Alvarez's personal background.\",\n",
    "    LLM=liteLLM_model\n",
    ")\n",
    "# Load the JSON file\n",
    "with open('data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters\n",
    "total_entries = len(data)\n",
    "yes_count = 0\n",
    "\n",
    "# Iterate through each entry in the JSON file\n",
    "for entry in data:\n",
    "    question = entry['question']\n",
    "    ground_truth = entry['ground_truth_answer']\n",
    "    generated = entry['generated_answer']\n",
    "    \n",
    "    # Use the function to check similarity\n",
    "    result = check_answer_similarity(\n",
    "        question=question,\n",
    "        ground_truth=ground_truth,\n",
    "        generated=generated,\n",
    "        LLM=liteLLM_model\n",
    "    )\n",
    "    \n",
    "    # Count 'yes' responses\n",
    "    if result.lower() == 'yes':\n",
    "        yes_count += 1\n",
    "\n",
    "# Calculate the proportion of 'yes' responses\n",
    "proportion_yes = yes_count / total_entries\n",
    "print(f\"Proportion of 'yes' responses: {proportion_yes}\")\n",
    "\n",
    "print(response)\n",
    "'''\n",
    "def analyze_similarity(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    liteLLM_model=\"gpt-4o-mini\"\n",
    "    total_entries = len(data)\n",
    "    yes_count = 0\n",
    "\n",
    "    for entry in data:\n",
    "        question = entry['question']\n",
    "        ground_truth = entry['ground_truth_answer']\n",
    "        generated = entry['generated_answer']\n",
    "        \n",
    "        result = check_answer_similarity(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            generated=generated,\n",
    "            LLM=liteLLM_model\n",
    "        )\n",
    "        \n",
    "        if \"yes\" in result.lower():\n",
    "            yes_count += 1\n",
    "\n",
    "    proportion_yes = yes_count / total_entries\n",
    "    print(f\"Proportion of 'yes' responses: {proportion_yes}\")\n",
    "    return proportion_yes,file_name\n",
    "\n",
    "# Example usage\n",
    "#proportion,file_name = analyze_similarity('data.json')\n",
    "\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "import concurrent.futures\n",
    "\n",
    "def process_j(j):\n",
    "    local_results = []\n",
    "    i = 0 + 1000 * j\n",
    "    for root, dirs, files in os.walk('./results'):\n",
    "        for file in files:\n",
    "            if file.startswith('evaluation') and file.endswith('.json'):\n",
    "                i += 1\n",
    "                file_path = os.path.join(root, file)\n",
    "                proportion, file_name = analyze_similarity(file_path)\n",
    "                local_results.append({'file_name': file_name, 'proportion': proportion})\n",
    "                print(\"file\" + str(i))\n",
    "    output_csv = f'results/QA_accuracy/comparison_results_{j}.csv'\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=['file_name', 'proportion'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(local_results)\n",
    "    print(f\"Results have been saved to {output_csv}\")\n",
    "    return local_results\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_j, j) for j in range(4)]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        results.extend(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for root, dirs, files in os.walk('./results'):\n",
    "    for file in files:\n",
    "        if file.startswith('evaluation') and file.endswith('.json'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            print(file_path)\n",
    "            c+=1\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HEART",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
